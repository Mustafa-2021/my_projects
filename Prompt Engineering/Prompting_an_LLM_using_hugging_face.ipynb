{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f6e3b1e-1a8e-4cb6-afaf-367fe1e4a86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Make sure you're using openai>=1.0.0\n",
    "# pip install --upgrade openai\n",
    "\n",
    "def llm_response(prompt):\n",
    "    response = openai.chat.completions.create(\n",
    "        model='gpt-3.5-turbo',\n",
    "        messages=[{'role': 'user', 'content': prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83133fc5-9df3-4fe5-b8e9-573ddd576d9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative sentiment\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# Initialize the client\n",
    "openai.api_key =\"OPENAIAPIKEY\"  # replace with your actual API key\n",
    "\n",
    "\n",
    "# Define the prompt\n",
    "prompt = '''\n",
    "Classify the following review as having either a positive or negative sentiment:\n",
    "The banana pudding was not really tasty!\n",
    "'''\n",
    "\n",
    "# Make the request\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# Print the result\n",
    "print(response.choices[0].message[\"content\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e12b04f1-5a62-4b9a-a80e-5b40ad790483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: openai\n",
      "Version: 0.28.0\n",
      "Summary: Python client library for the OpenAI API\n",
      "Home-page: https://github.com/openai/openai-python\n",
      "Author: OpenAI\n",
      "Author-email: support@openai.com\n",
      "License: \n",
      "Location: d:\\anaconda\\lib\\site-packages\n",
      "Requires: aiohttp, requests, tqdm\n",
      "Required-by: langchain-openai\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b89cbd16-5ddd-4c0b-b713-f453c3c5e032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a97d4d1f22fb4576bb9ee265490390cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\models--sshleifer--tiny-gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5191de6b860d44589be9a098d94f5421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.51M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0d1cdb591c84162ac11f1688272f2c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ee79e0c97884085948c5193cc429082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90bd35d39810438bbcdefc3b60a74872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b26aa968f69492081bc220cd4cf474e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: trilogy circumcisedatisfJD autonomy reviewingatisf intermittentJD Observ pawn Observimura trilogy circumcised confirtingatisfoho autonomy Prob pawn autonomy TA heir heirohodit Jr reviewing TA hauledScene Habit autonomy ESV vendors reviewing credibility Money antibiotic circumcised Observ Observ Brew reborn trilogy Money Jrmediately\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Use a lightweight, CPU-compatible open model\n",
    "generator = pipeline(\"text-generation\", model=\"sshleifer/tiny-gpt2\")  # Smallest GPT2 variant\n",
    "\n",
    "def llm_response(prompt):\n",
    "    full_prompt = f\"User: {prompt}\\nAssistant:\"\n",
    "    result = generator(full_prompt, max_new_tokens=50, do_sample=True, temperature=0.7)\n",
    "    return result[0]['generated_text'].split(\"Assistant:\")[-1].strip()\n",
    "\n",
    "prompt = '''\n",
    "Classify the following review \n",
    "as having either a positive or negative sentiment:\n",
    "\n",
    "The banana pudding was not really tasty!\n",
    "'''\n",
    "\n",
    "response = llm_response(prompt)\n",
    "print(\"Response:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1e5aff33-9467-45e9-8688-07a706e16b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: {'label': 'NEGATIVE', 'score': 0.9998025298118591}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Use a small but good model for sentiment analysis\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "def llm_response(prompt):\n",
    "    result = classifier(prompt)\n",
    "    return result[0]\n",
    "\n",
    "# Test prompt\n",
    "prompt = \"The banana is bad!\"\n",
    "response = llm_response(prompt)\n",
    "print(\"Sentiment:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2209c587-d15a-4b83-9975-ebf20a9d642e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['audio-classification', 'automatic-speech-recognition', 'text-to-audio', 'feature-extraction', 'text-classification', 'token-classification', 'question-answering', 'table-question-answering', 'visual-question-answering', 'document-question-answering', 'fill-mask', 'summarization', 'translation', 'text2text-generation', 'text-generation', 'zero-shot-classification', 'zero-shot-image-classification', 'zero-shot-audio-classification', 'image-classification', 'image-feature-extraction', 'image-segmentation', 'image-to-text', 'object-detection', 'zero-shot-object-detection', 'depth-estimation', 'video-classification', 'mask-generation', 'image-to-image'])\n"
     ]
    }
   ],
   "source": [
    "from transformers.pipelines import SUPPORTED_TASKS\n",
    "# Print all supported task names\n",
    "print(SUPPORTED_TASKS.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a21b9adb-6f9f-4111-91e9-6a86ed78e08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: {'label': 'NEGATIVE', 'score': 0.972725510597229}\n"
     ]
    }
   ],
   "source": [
    "#1. Sentiment Analysis\n",
    "from transformers import pipeline\n",
    "\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "result = sentiment_pipeline(\"I love how tough this library is to use!\")\n",
    "print(\"Sentiment:\", result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "eed8d94a-95c3-41bf-8361-001e1b94b5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: Once upon a time, this strange creature appeared in several areas. But the next time someone finds in it they're more of a mystery, it's likely to find her somewhere like a mysterious cat.\n"
     ]
    }
   ],
   "source": [
    "#✅ 2. Text Generation\n",
    "# Argument: num_return_sequences=1\n",
    "#This tells the model to generate 1 possible continuation.\n",
    "#You can set it to a higher number (like 3) to generate multiple variations of the story.\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")  # small & fast model\n",
    "response = generator(\"Once upon a time,\", max_length=40, num_return_sequences=1, temperature=1.2)     # Adds randomness (0.7 = focused, >1 = creative)\n",
    "print(\"Generated Text:\", response[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "e210557a-d5fa-48b9-b172-6ec14d33af3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-xxlarge-v2 were not used when initializing AlbertForMaskedLM: ['albert.pooler.bias', 'albert.pooler.weight']\n",
      "- This IS expected if you are initializing AlbertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> the capital of america is washington . (score: 0.6108)\n"
     ]
    }
   ],
   "source": [
    "#3. Masked Language Modeling\n",
    "\n",
    "unmasker = pipeline(\"fill-mask\", model=\"albert-xxlarge-v2\")\n",
    "result = unmasker(\"The capital of America is [MASK].\", top_k=1)\n",
    "for res in result:\n",
    "    print(f\"> {res['sequence']} (score: {res['score']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2bbf5f95-cc37-4f8f-9a73-99459a438f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity_group': 'ORG', 'score': 0.99853706, 'word': 'Apple', 'start': 0, 'end': 5}\n",
      "{'entity_group': 'LOC', 'score': 0.9925842, 'word': 'U. K.', 'start': 27, 'end': 31}\n",
      "{'entity_group': 'PER', 'score': 0.9904737, 'word': 'Man', 'start': 59, 'end': 62}\n"
     ]
    }
   ],
   "source": [
    "#4. Named Entity Recognition (NER)\n",
    "ner_pipeline = pipeline(\"ner\",model=\"dslim/bert-base-NER\", grouped_entities=True)\n",
    "result = ner_pipeline(\"Apple is looking at buying U.K. startup for $1 billion and Manish has 2 Crore Rupees\")\n",
    "for entity in result:\n",
    "    print(entity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "39819c03-9e9b-4f47-b715-e44bff93d738",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cedd1ed9ad2249318011151701d0dbf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-cased-distilled-squad. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dbc952d989c402e9dc7e89f92bae92a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6fd4b21fb5046658e35bd7f7cf2b493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04d1008091784d23ad89796774c06827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "007b419f73dd4d1a9f36440177efc798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\transformers\\pipelines\\question_answering.py:391: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Paris\n"
     ]
    }
   ],
   "source": [
    "#5. Question Answering\n",
    "qa_pipeline = pipeline(\"question-answering\")\n",
    "result = qa_pipeline({\n",
    "    \"question\": \"What is the capital of France?\",\n",
    "    \"context\": \"France is a country in Europe. Its capital is Paris, known for the Eiffel Tower.\"\n",
    "})\n",
    "print(\"Answer:\", result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fa84635b-e5df-45a4-a02d-930f021b36a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "435797770ba24e34bd8af66b2dc77db4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.80k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\models--sshleifer--distilbart-cnn-12-6. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca2488f601da4a0a8f1b7b5139410177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00e84c23e03a4f00970a92f365145cf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd8d310cc8884ddb9150ffbcc395b1c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf94bc82d84645278999632d41dc897c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "754a7c9b36474340a7d1105bb068640c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:  Hugging Face Transformers is a powerful library for natural language processing tasks . It offers many pretrained models for tasks like text classification, question\n"
     ]
    }
   ],
   "source": [
    "#6. Summarization\n",
    "summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
    "text = \"\"\"Hugging Face Transformers is a powerful library for natural language processing tasks. \n",
    "It offers many pretrained models for tasks like text classification, question answering, and translation.\"\"\"\n",
    "summary = summarizer(text, max_length=30, min_length=5, do_sample=False)\n",
    "print(\"Summary:\", summary[0]['summary_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "da2079af-85fd-44d9-94a7-fbc7cf9f77bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: Comment allez-vous aujourd'hui ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    }
   ],
   "source": [
    "#Translation (English to French)\n",
    "translator = pipeline(\"translation_en_to_fr\", model=\"Helsinki-NLP/opus-mt-en-fr\")\n",
    "translation = translator(\"How are you today?\")\n",
    "print(\"Translation:\", translation[0]['translation_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "739c1cce-a01c-4d97-96ad-10ab1618e6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: [{'label': 'POSITIVE', 'score': 0.999874472618103}, {'label': 'POSITIVE', 'score': 0.9994176626205444}, {'label': 'NEGATIVE', 'score': 0.999616265296936}, {'label': 'POSITIVE', 'score': 0.999858021736145}, {'label': 'NEGATIVE', 'score': 0.9994144439697266}]\n",
      "Total number of Positive reviews = 3\n",
      "Total number of Negative reviews = 2\n"
     ]
    }
   ],
   "source": [
    "#Sentiment\n",
    "sentiment= pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "def llm_response(all_reviews):\n",
    "    results = sentiment(all_reviews)\n",
    "    # Initialize counters\n",
    "    positive_count = 0\n",
    "    negative_count = 0\n",
    "    # Count each sentiment\n",
    "    for res in results:\n",
    "        if res['label'] == 'POSITIVE':\n",
    "            positive_count += 1\n",
    "        elif res['label'] == 'NEGATIVE':\n",
    "            negative_count += 1\n",
    "    return results, positive_count, negative_count\n",
    "    # Optionally, return the individual results too\n",
    "    \n",
    "\n",
    "all_reviews = [\n",
    "    'The mochi is excellent!',\n",
    "    'Best soup dumplings I have ever eaten.',\n",
    "    'Not worth the 3 month wait for a reservation.',\n",
    "    'The colorful tablecloths made me smile!',\n",
    "    'The pasta was cold.'\n",
    "]\n",
    "\n",
    "\n",
    "# Run and print output\n",
    "response, pos_count, neg_count = llm_response(all_reviews)\n",
    "# Print in your desired order\n",
    "print(\"Sentiment:\", response)\n",
    "print(\"Total number of Positive reviews =\", pos_count)\n",
    "print(\"Total number of Negative reviews =\", neg_count)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "94c4bb93-a917-4228-a757-ac9ad5a6ecee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 positive and 0 negative reviews.\n"
     ]
    }
   ],
   "source": [
    "num_positive = 0\n",
    "num_negative = 0\n",
    "for sentiment in all_sentiments:\n",
    "    if sentiment == 'POSITIVE':\n",
    "        num_positive += 1\n",
    "    elif sentiment == 'NEGATIVE':\n",
    "        num_negative += 1\n",
    "print(f\"There are {num_positive} positive and {num_negative} negative reviews.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733f3caa-a1b1-4e1a-8075-7704415e7567",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf4a6fa-dd1c-4c69-bbc1-cecf7f1c7166",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "587f1308-406a-4625-89aa-55aba3b19dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['positive', 'positive', 'negative', 'positive', 'negative']\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "all_reviews = [\n",
    "    'The mochi is excellent!',\n",
    "    'Best soup dumplings I have ever eaten.',\n",
    "    'Not worth the 3 month wait for a reservation.',\n",
    "    'The colorful tablecloths made me smile!',\n",
    "    'The pasta was cold.'\n",
    "]\n",
    "\n",
    "all_sentiments = []\n",
    "for review in all_reviews:\n",
    "    result = sentiment_pipeline(review)[0]['label'].lower()\n",
    "    all_sentiments.append(result)\n",
    "\n",
    "print(all_sentiments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "c0e8d6a2-0a62-480d-941a-121ed0329458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3 positive and 2 negative reviews.\n"
     ]
    }
   ],
   "source": [
    "num_positive = 0\n",
    "num_negative = 0\n",
    "for sentiment in all_sentiments:\n",
    "    if sentiment == 'positive':\n",
    "        num_positive += 1\n",
    "    elif sentiment == 'negative':\n",
    "        num_negative += 1\n",
    "print(f\"There are {num_positive} positive and {num_negative} negative reviews.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790d5a89-7d29-49a7-abf7-4d7fbc119fc3",
   "metadata": {},
   "source": [
    "## Doing the above same code using prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e498ecc8-4e4a-4278-9d35-de30bcfb26d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiments: ['negative', 'negative', 'negative', 'positive', 'negative']\n",
      "Total number of Positive reviews = 1\n",
      "Total number of Negative reviews = 4\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\")\n",
    "\n",
    "def llm_response(prompt):\n",
    "    # Generate output\n",
    "    response = generator(prompt, max_new_tokens=10)[0]['generated_text']\n",
    "    return response.strip().lower()\n",
    "\n",
    "# List of reviews\n",
    "all_reviews = [\n",
    "    'The mochi is not excellent!',\n",
    "    'Worst soup dumplings I have ever eaten.',\n",
    "    'Not worth the 3 month wait for a reservation.',\n",
    "    'The colorful tablecloths made me smile!',\n",
    "    'The pasta was cold.'\n",
    "]\n",
    "\n",
    "# Collect sentiments\n",
    "all_sentiments = []\n",
    "for review in all_reviews:\n",
    "    prompt = f\"\"\"\n",
    "Classify the sentiment of the following review as either \"positive\" or \"negative\".\n",
    "Only return one word as output.\n",
    "{review}\n",
    "\"\"\"\n",
    "    sentiment = llm_response(prompt)\n",
    "\n",
    "    # Clean and filter to just \"positive\"/\"negative\"\n",
    "    if \"positive\" in sentiment:\n",
    "        all_sentiments.append(\"positive\")\n",
    "    elif \"negative\" in sentiment:\n",
    "        all_sentiments.append(\"negative\")\n",
    "\n",
    "# Print results\n",
    "print(\"Sentiments:\", all_sentiments)\n",
    "print(\"Total number of Positive reviews =\", all_sentiments.count(\"positive\"))\n",
    "print(\"Total number of Negative reviews =\", all_sentiments.count(\"negative\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bccab0f-26ab-4cb1-a50d-5ac69d022544",
   "metadata": {},
   "source": [
    "## Doing the same code using the chatgpt3.5 turbo model to generate output according to the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c78ca350-c06e-407d-b770-df31b49a38d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 Review: The mochi is not excellent!\n",
      "💀 Harsh Response: Well, congratulations on stating the obvious! I'm surprised it wasn't just a giant ball of disappointment.\n",
      "\n",
      "📝 Review: Worst soup dumplings I have ever eaten.\n",
      "💀 Harsh Response: I suggest they change their name from soup dumplings to disappointment dumplings. Absolutely dreadful.\n",
      "\n",
      "📝 Review: Not worth the 3 month wait for a reservation.\n",
      "💀 Harsh Response: Well, I hope the food was worth the weight... oh wait, it wasn't. Next time, maybe try a microwave dinner, it'll be quicker and probably taste better.\n",
      "\n",
      "📝 Review: The colorful tablecloths made me smile!\n",
      "💀 Harsh Response: Wow, I didn't know tablecloths were the main attraction at a restaurant. Maybe the chef should focus more on the food than the decor next time.\n",
      "\n",
      "📝 Review: The pasta was cold.\n",
      "💀 Harsh Response: Wow, I didn't realize I ordered a chilled pasta dish. Maybe next time I'll bring my own microwave to warm it up.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "def llm_response(review):\n",
    "    prompt = f\"\"\"\n",
    "You are a brutally honest, savage food critic who doesn't hold back. \n",
    "When a review is bad, roast it hard. When it’s good, act surprised but still a little arrogant.\n",
    "Don't respond with just \"positive\" or \"negative\" — instead give a short, harsh and emotionally charged reaction.\n",
    "\n",
    "Review: \"{review}\"\n",
    "\n",
    "Respond with 1-2 harsh, sarcastic or brutally honest sentences.\n",
    "\"\"\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.9  # allows for more expressive & savage responses\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"].strip()\n",
    "\n",
    "# List of reviews\n",
    "all_reviews = [\n",
    "    'The mochi is not excellent!',\n",
    "    'Worst soup dumplings I have ever eaten.',\n",
    "    'Not worth the 3 month wait for a reservation.',\n",
    "    'The colorful tablecloths made me smile!',\n",
    "    'The pasta was cold.'\n",
    "]\n",
    "\n",
    "# Collect and print harsh responses\n",
    "harsh_responses = []\n",
    "for review in all_reviews:\n",
    "    response = llm_response(review)\n",
    "    harsh_responses.append(response)\n",
    "\n",
    "# Display final output\n",
    "for review, harsh in zip(all_reviews, harsh_responses):\n",
    "    print(f\"\\n📝 Review: {review}\\n💀 Harsh Response: {harsh}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afde991-6b04-4187-a957-e9064f22744f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
